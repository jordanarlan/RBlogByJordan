---
title: Supervised machine learning used to classify plants
author: Jordan Arlan
date: '2019-04-03'
slug: supervised-machine-learning-used-to-classify-plants
categories:
  - Machine learning
tags:
  - Machine learning
images: ~
---


Today we are going to look at supervised learning in R using the packages 'class', 'tidyverse', and 'caret'. The goal of this model is to classify plants in to a certain species based off of the dimensions of the petals and sepals. 


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library('class')
library(tidyverse)
library('caret')
options(digits=2)
```


##1. Cleaning the data

The first things I need to do in order to create a model is clean the data. This can include many different steps depending on how much needs to be done. Luckily for this data set only a minimal amount is needed.

```{r}
str(iris)
```



```{r}

iris.ml <- iris %>% mutate(Sepal.tot = Sepal.Length*Sepal.Width, 
                           Petal.tot = Petal.Length*Petal.Width)

##Now I will normalize the data using the scale() function.  
iris.ml[-5] <- scale(iris.ml[-5])

## I am checking to see that the data is now normalized 
## (mean == 0 & sd == 1)
table("mean" = colMeans(iris.ml[-5]),"sd" =  apply(iris.ml[-5],2,sd))


```

##2. Exploratory data analysis 
First I am going to start with some EDA (exploratory data analysis). I’m going to create a scatterplot.

```{r}

ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point() 

ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()

ggplot(iris.ml, aes(Petal.tot, Sepal.tot, color = Species)) + geom_point()
```



The plots show defined clusters. The second plot looking at petal dimensions has extremely well defined clusters. This indicates that the variables being plotted are good indicators at what species a plant is. 

##3. Preparing the data for modeling 

Now I will make a test and training subset of the data.


```{r}

#Checking the proportions of each factor level
prop.table(table(iris$Species))

#Now to create a variable called test that indicates if the data point is in the training or test set
set.seed(12345)
test <- createDataPartition(iris$Species, p = 0.333, list = FALSE)

# Checking to see that the two data sets are similar in distribution. 
summary(iris[test, ])
summary(iris[-test, ])

```

```{r}
# I am going to create a vector containing all the labels of species
iris.ml.test <- iris.ml[test, ]
iris.ml.train <- iris.ml[-test, ]
species.train.label <- iris.ml.train$Species

```


##4. Supervised machine learning: classification with K nearest neighbors

The machine learning method I am going to use is k nearest neighbors. This algorithm maps the variables of interest to a feature space and compares the data to its k nearest neighbors in order to minimize the Euclidian distance within clusters given the parameters set .I’m using the package 'class' and the function knn().

```{r}

#I will start off by setting k = 1 this number is related on how many neighbors are included in the classification calculation. 
iris.pred.knn <- knn(iris.ml.train[-5], iris.ml.test[-5], 
                     cl = species.train.label, k = 1 )

#In order to see how well this model performed I am going to make a table. 
species.test.label <- iris.ml.test$Species

table(iris.pred.knn, species.test.label)

```



###4.1 Picking k

In order to find the right k we need to decide how we will be comparing them. I think a measure of accuracy via comparing predictions to actual labels will work well. 

```{r}
set.seed(12345)
results <- map_dbl(1:10, function(k){
  model <- knn(iris.ml.train[-5], iris.ml.test[-5], 
                     cl = species.train.label, k = k)
  
  mean(model == species.test.label)})

result_df <- data.frame(
  k = 1:10,
  results = results
)

result_df %>% 
  ggplot(aes(x = k, y = results))+ 
  geom_line() +
  scale_x_continuous(breaks = 1:10)
```

Picking k is up to the one making the model but in theory you want the lowest K with out giving up too much accuracy. I am going to go with 5 however 7 could be a good choice too. 

```{r}

#I will now set k = 5
iris.pred.knn.5 <- knn(iris.ml.train[-5], iris.ml.test[-5], 
                     cl = species.train.label, k = 5 )

#In order to see how well this model performed I am going to make a table. 
species.test.label <- iris.ml.test$Species

table(iris.pred.knn.5, species.test.label)

mean(iris.pred.knn.5 == species.test.label)

```



##5. Results 

So we have now achieved a classification accuracy of `r mean(iris.pred.knn.5 == species.test.label)*100`% that of course is extremely good. However, I would be worried about potential over fitting of the data given that we don’t have a very large data set. 


In order to get better insight in to the confidence of each classification prediction we can set the prob = TRUE.

```{r}
iris.pred.knn.5 <- knn(iris.ml.train[-5], iris.ml.test[-5], cl = species.train.label, k = 5 , prob = TRUE)

#Now to extract the "prob" attribute from the knn object
iris.prob.knn <- attr(iris.pred.knn.5, "prob")

mean(iris.prob.knn)
```

So on average the model is `r mean(iris.prob.knn)*100`% sure about its classification choices. 

